{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2441, 5)\n",
      "X_test shape: (611, 5)\n",
      "y_train shape: (2441,)\n",
      "y_test shape: (611,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Lista de las columnas que quieres seleccionar\n",
    "columnas_a_cargar = ['SDT', 'pH_CAMPO', 'OD_%', 'TEMP_AMB', 'TEMP_AGUA', 'N_TOT']  # Cambia los nombres seg√∫n tus columnas\n",
    "\n",
    "# Lee el archivo CSV y lo convierte en un DataFrame\n",
    "df = pd.read_csv('C:\\\\Users\\\\Alienware X15\\\\Desktop\\\\tesis\\\\BDreconstruccion\\\\BDWeka\\\\BDentrenamientoWeka.csv', usecols=columnas_a_cargar)\n",
    "\n",
    "# Asignar las variables de entrada (X) y la variable de salida (y)\n",
    "X = df.drop(columns=['N_TOT'])  # Todas las columnas excepto 'N_TOT'\n",
    "y = df['N_TOT']  # La columna 'N_TOT' como variable de salida\n",
    "\n",
    "# Normalizar los datos de entrada antes de dividir\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verificar las formas\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor: tensor([[-0.1895, -0.3654, -0.3355,  1.4303,  1.6370],\n",
      "        [-0.2037,  0.1371,  0.1490, -0.3182,  0.4562],\n",
      "        [-0.1632,  3.2478,  1.3398,  0.6954,  0.8306],\n",
      "        ...,\n",
      "        [ 0.1147, -0.8201, -0.0337, -0.0648, -0.1198],\n",
      "        [-0.1680,  0.6156,  1.0070, -1.3571, -0.4654],\n",
      "        [-0.1902,  0.8549,  0.6938,  0.1886,  0.1682]])\n",
      "X_test_tensor: tensor([[-0.2044, -0.4851, -0.0141,  1.0755,  0.0242],\n",
      "        [-0.1856,  1.0942,  0.9972, -0.5716, -1.4735],\n",
      "        [-0.1899,  0.3763, -0.3795,  0.6954,  0.3698],\n",
      "        ...,\n",
      "        [-0.1199,  0.1371, -0.1805, -0.6729,  0.4274],\n",
      "        [-0.0262, -0.4372, -2.0401, -1.2811,  0.2546],\n",
      "        [-0.1588,  0.1371,  0.0087,  0.4420,  1.2626]])\n",
      "y_train_tensor: tensor([0.4280, 0.7445, 0.8307,  ..., 0.7951, 0.8910, 0.6638])\n",
      "y_test_tensor: tensor([3.8000e-01, 5.1666e-01, 8.5000e-01, 5.2024e-01, 2.6370e+00, 4.1800e-01,\n",
      "        8.4588e-01, 2.8112e+00, 4.6019e-01, 1.8816e+00, 2.9721e+00, 6.4029e+00,\n",
      "        3.6761e-01, 3.0910e+00, 5.1296e-01, 1.3017e+00, 5.4100e-01, 2.0800e-01,\n",
      "        3.2947e-01, 6.1400e-01, 1.2703e+00, 1.6400e+00, 8.4680e-01, 3.9456e-01,\n",
      "        5.4300e-01, 3.1520e-01, 1.1349e+01, 1.0785e+00, 9.0800e-01, 1.6620e+00,\n",
      "        5.9800e-01, 6.7160e-01, 3.7565e+01, 4.3090e-01, 5.6576e-01, 1.1312e+00,\n",
      "        6.6411e+00, 7.2576e-01, 3.9441e+00, 3.6560e+00, 1.5450e+00, 3.2603e+01,\n",
      "        9.3200e-01, 1.5440e+00, 2.8794e+01, 1.6726e+00, 4.1233e-01, 1.0601e+00,\n",
      "        3.6989e-01, 4.7049e-01, 4.8002e+00, 4.3710e-01, 3.9539e-01, 1.9180e-01,\n",
      "        4.7300e-01, 1.5029e+01, 1.8225e+01, 2.4659e+00, 1.0375e+00, 3.5810e+00,\n",
      "        6.6770e-01, 5.7701e-01, 1.8709e-01, 4.8472e-01, 2.8990e-01, 5.3610e-01,\n",
      "        2.2522e+01, 4.7997e+01, 3.1878e+00, 2.3966e+00, 6.6645e-01, 6.4628e-01,\n",
      "        4.7224e+00, 3.9739e-01, 2.4640e-01, 7.3655e-01, 2.0120e+00, 4.2400e-01,\n",
      "        1.0842e+00, 7.8500e-01, 8.7015e+00, 3.2295e-01, 6.0097e+00, 1.1882e+00,\n",
      "        4.3795e-01, 4.0694e-01, 5.1132e-01, 3.7324e-01, 1.7078e+00, 1.8776e+00,\n",
      "        2.2730e+00, 3.6621e+01, 8.6490e-01, 6.9415e-01, 7.4596e-01, 1.9464e-01,\n",
      "        1.9305e+00, 2.8141e-01, 6.1326e-01, 4.2211e+00, 4.7858e-01, 4.4309e+01,\n",
      "        5.7660e-01, 2.7818e-01, 9.1942e-01, 1.8940e+00, 2.0573e+00, 1.1379e+00,\n",
      "        2.7796e+01, 9.5000e-03, 1.1049e+00, 4.5181e+00, 1.7540e+00, 5.2602e-01,\n",
      "        7.0218e+00, 1.9358e+00, 1.3882e+00, 3.7769e-01, 3.6401e-01, 4.5600e-01,\n",
      "        3.1090e-01, 7.5093e-01, 1.6828e-01, 5.1161e+00, 3.5482e+00, 1.2628e-01,\n",
      "        2.3580e-01, 5.2671e-01, 9.2028e-01, 3.2570e-01, 6.6171e-01, 3.7343e-01,\n",
      "        5.0749e+00, 2.6980e+00, 4.0215e+00, 9.8469e-01, 4.8649e-01, 4.1781e-01,\n",
      "        1.4051e+00, 1.0949e+00, 4.9800e+00, 1.0109e+00, 1.2970e+00, 2.8600e-01,\n",
      "        2.5050e+00, 2.7220e+00, 2.6340e+00, 2.1135e+00, 6.5636e-01, 1.3459e+00,\n",
      "        8.9700e-01, 1.4551e+00, 1.0214e+01, 7.8958e+01, 3.4590e+01, 9.4771e-01,\n",
      "        7.3820e-01, 1.1194e+00, 6.2397e-01, 4.6657e-01, 1.0720e+00, 6.9800e-01,\n",
      "        5.1510e+00, 3.8022e+00, 7.6923e-01, 3.4098e+00, 7.0935e-01, 4.9825e-01,\n",
      "        4.6929e-01, 8.6229e-01, 1.1280e+00, 1.0266e+00, 2.8190e+00, 6.2930e-01,\n",
      "        1.1750e+00, 1.4683e+00, 1.8744e-01, 6.1202e+01, 3.3550e-01, 3.4100e-01,\n",
      "        3.7510e+00, 3.3881e+00, 3.0040e+00, 1.5559e+00, 3.4543e+01, 3.3100e-01,\n",
      "        1.8653e+00, 6.2150e-01, 9.2100e-01, 1.9132e+00, 4.9993e-01, 1.5808e+00,\n",
      "        2.4135e+00, 1.0430e+00, 1.1215e+00, 6.5558e-01, 1.3915e+00, 4.7445e-01,\n",
      "        7.5736e-01, 1.7156e+00, 3.7820e+00, 1.1408e+00, 6.6924e-01, 6.7414e-01,\n",
      "        1.7233e+01, 5.2924e-01, 5.2044e-01, 9.2600e-01, 3.1102e-01, 3.7426e-01,\n",
      "        1.7632e+00, 1.7645e+00, 6.7144e-01, 1.2546e+00, 7.6410e-01, 1.7095e+01,\n",
      "        1.4061e+00, 1.4312e+01, 2.3580e+00, 4.6608e-01, 1.2652e+00, 9.1900e-01,\n",
      "        1.2326e+00, 3.3200e-01, 1.6563e+00, 3.0730e-01, 3.9787e-01, 1.4560e+01,\n",
      "        2.3300e-01, 1.2573e+00, 6.5359e-01, 1.2185e+01, 2.7470e+00, 1.0185e+01,\n",
      "        6.4300e-01, 2.6720e+00, 5.4360e-01, 5.5850e-01, 7.6145e-01, 2.9100e-01,\n",
      "        8.7600e-01, 1.0300e+00, 2.2904e+00, 7.3400e-01, 1.5145e+00, 6.8105e-01,\n",
      "        2.0570e+00, 5.2553e-01, 1.9127e+00, 2.2336e+00, 1.5334e+00, 1.4963e+00,\n",
      "        1.0205e+00, 8.8349e-01, 3.7026e+01, 1.4609e+01, 6.0681e+00, 9.9680e-01,\n",
      "        9.1411e-01, 2.1100e-01, 9.6310e+00, 4.7570e-01, 1.6530e+00, 5.1244e-01,\n",
      "        4.8865e-01, 1.0172e+00, 1.6881e+00, 1.3651e+00, 1.4528e+00, 2.9420e+00,\n",
      "        5.8391e-01, 2.1780e+00, 2.6700e-01, 3.8959e+00, 9.1189e-01, 5.7634e-01,\n",
      "        2.2103e+00, 4.9840e-01, 1.7524e+01, 6.2350e+00, 5.2830e+00, 8.0412e-01,\n",
      "        9.8200e-01, 2.8914e+00, 1.3324e+00, 6.9117e-01, 4.0639e+00, 1.0305e+01,\n",
      "        1.1590e+00, 8.9704e-01, 8.3200e-01, 2.0317e+00, 1.6056e+00, 7.5820e-01,\n",
      "        2.2676e-01, 2.3885e+00, 1.9251e+00, 6.6300e-01, 5.8189e-01, 9.1950e-01,\n",
      "        8.4896e-01, 6.9797e-01, 5.1241e+00, 1.2413e+00, 1.5178e+01, 9.6135e-01,\n",
      "        4.1173e-01, 4.3762e+01, 2.8310e-01, 4.8286e-01, 1.0353e+00, 7.7555e-01,\n",
      "        8.8985e+00, 5.2058e-01, 1.1961e+00, 7.9043e-01, 1.8170e+00, 1.2576e+00,\n",
      "        6.3668e-01, 3.1400e-01, 1.3866e+01, 3.1863e+00, 4.0713e+00, 4.2300e-01,\n",
      "        5.4300e-01, 3.2290e-01, 4.5745e-01, 4.6661e-01, 9.5487e-01, 9.3261e-01,\n",
      "        2.5720e+00, 6.7814e+00, 1.1459e+00, 2.0896e+00, 3.9965e+00, 1.6761e+00,\n",
      "        1.2521e+00, 2.9920e-01, 5.4571e-01, 1.6402e+00, 4.7870e+00, 1.8948e+00,\n",
      "        1.4844e+00, 2.1193e+00, 1.6350e-01, 9.6176e-01, 1.7135e+01, 6.4640e-01,\n",
      "        3.1057e-01, 1.5280e+00, 3.9710e-01, 4.1479e-01, 3.6920e+00, 8.3655e-01,\n",
      "        9.8000e-01, 5.8676e+00, 8.4577e-01, 8.2012e-01, 7.4692e+00, 5.5110e-01,\n",
      "        7.9691e-01, 2.4613e+00, 1.0129e+00, 1.2254e+01, 1.7560e+00, 2.1822e-01,\n",
      "        7.2719e-01, 3.5784e+00, 9.4068e-01, 4.6892e+00, 2.2962e+00, 8.5943e-01,\n",
      "        1.1578e+00, 9.0730e+00, 1.1230e+00, 7.8674e-01, 5.7665e-01, 8.5161e-01,\n",
      "        1.5581e+00, 5.6153e-01, 4.5360e-01, 2.4029e+00, 3.7912e+01, 7.7600e-01,\n",
      "        5.9900e-01, 1.1269e+00, 4.1328e-01, 4.5950e+00, 1.2189e+00, 1.7268e+01,\n",
      "        8.3363e-01, 1.1441e+00, 3.0115e+01, 1.4838e+00, 6.6529e+00, 1.2341e+00,\n",
      "        3.6388e-01, 7.5182e-01, 4.9330e-01, 3.8852e+01, 1.6915e+01, 1.6074e+01,\n",
      "        8.3012e-01, 1.5801e+00, 8.2000e-01, 9.6400e-01, 7.1751e-01, 8.8440e-01,\n",
      "        8.1995e-01, 1.1716e+00, 1.2500e+00, 2.6870e+00, 6.3800e-01, 1.8376e+01,\n",
      "        3.0223e+00, 5.4800e-01, 2.7335e+00, 1.3866e+00, 5.9555e+00, 7.0048e-01,\n",
      "        3.4030e+01, 4.5728e-01, 1.5228e+00, 7.0146e-01, 2.5320e+01, 3.9542e+00,\n",
      "        2.5358e-01, 7.4820e-01, 1.5249e+00, 5.4700e-01, 7.6249e-01, 4.1064e-01,\n",
      "        7.6993e-01, 2.7334e+00, 3.5250e+00, 9.8146e-01, 7.4200e-01, 4.5136e-01,\n",
      "        1.7278e+01, 6.4000e-01, 1.5797e+00, 9.8550e-01, 6.0956e-01, 1.1407e+00,\n",
      "        4.4620e-01, 2.1950e+00, 7.6573e-01, 1.0846e+00, 5.2530e-01, 1.1240e+00,\n",
      "        1.0823e+00, 1.8487e+00, 1.6553e+00, 1.5599e+00, 2.1245e-01, 1.2311e+00,\n",
      "        2.2270e+01, 2.6593e+00, 6.4090e-01, 2.0100e-01, 3.6222e+00, 9.1575e-01,\n",
      "        9.4677e-01, 1.0561e+00, 2.9777e+01, 5.3620e-01, 9.6530e-01, 4.0300e-01,\n",
      "        1.5276e+00, 1.8940e+01, 1.7265e+00, 1.0433e+00, 8.6589e-01, 8.5187e-01,\n",
      "        4.4850e-01, 1.1363e+00, 4.8490e-01, 2.6785e+01, 5.1160e-01, 1.9598e+00,\n",
      "        1.5953e+00, 8.2010e+00, 7.2700e-01, 7.4061e-01, 6.9079e-01, 2.6090e+00,\n",
      "        2.1724e+00, 1.5455e+00, 1.4184e+00, 5.8310e+00, 1.1635e+00, 4.6771e-01,\n",
      "        1.7628e+00, 2.6428e-01, 1.7440e+00, 2.0460e+00, 4.8121e+00, 6.2943e-01,\n",
      "        1.1020e+00, 4.0620e-01, 7.4462e-01, 3.6062e-01, 8.1910e-01, 1.1782e+00,\n",
      "        2.1180e+00, 2.4742e+01, 5.8134e-01, 2.5600e-01, 7.0620e+00, 3.7112e+00,\n",
      "        4.5570e-01, 3.4907e-01, 5.9461e-01, 6.8450e-01, 8.4846e-01, 5.6607e-01,\n",
      "        4.8300e-01, 1.9458e+00, 1.0477e+00, 2.1689e+01, 6.6553e-01, 6.0606e-01,\n",
      "        7.3598e-01, 8.8088e-01, 5.6692e+00, 7.3871e+00, 4.3400e-01, 2.7848e-01,\n",
      "        1.0748e+00, 2.1691e+00, 1.4407e+01, 2.5203e+00, 4.3588e-01, 1.1839e+00,\n",
      "        1.6182e-01, 6.5070e+00, 1.4930e-01, 1.2512e+00, 6.6842e-01, 3.0684e+00,\n",
      "        4.5810e-01, 1.6407e+00, 4.7630e-01, 1.8576e-01, 2.3200e-01, 6.5310e-01,\n",
      "        6.1500e-01, 3.8720e-01, 1.3305e+00, 1.9540e+00, 2.6709e+00, 2.0784e+01,\n",
      "        4.0837e+00, 5.1252e-01, 9.6143e-01, 3.7480e+00, 3.0370e+00, 3.3111e+00,\n",
      "        1.9980e+00, 8.6004e-01, 5.4481e-01, 6.9900e-01, 2.6627e+01, 1.3135e+00,\n",
      "        8.4335e-01, 2.2234e+00, 5.7079e+00, 1.5140e+00, 3.5321e-01, 5.1638e-01,\n",
      "        1.3137e+00, 2.0810e+00, 1.4533e+00, 9.1070e-01, 2.0434e+00, 3.5580e+01,\n",
      "        1.1230e+00, 5.8601e-01, 5.7654e-01, 1.2033e+00, 3.8100e-01, 5.0874e-01,\n",
      "        1.2045e+00, 1.1801e+00, 1.0061e+00, 3.8411e+00, 4.2700e-01, 7.7169e-01,\n",
      "        5.3052e+00, 2.7967e+00, 6.5668e-01, 5.1356e+00, 2.1235e+01, 1.9899e+00,\n",
      "        5.0109e+00, 8.0270e-01, 3.0475e-01, 1.6813e+00, 1.4440e+00, 4.9206e+00,\n",
      "        6.0450e-01, 6.0200e-01, 2.8188e+00, 8.9130e-01, 2.3955e+00, 1.3560e+00,\n",
      "        4.3600e-01, 6.5757e-01, 2.6135e+00, 2.5448e+01, 3.1589e+00])\n"
     ]
    }
   ],
   "source": [
    "# Convertir los datos a tensores\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)  # Convertir a valores para asegurar compatibilidad\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Imprimir c√≥mo se ven los tensores\n",
    "print(\"X_train_tensor:\", X_train_tensor)\n",
    "print(\"X_test_tensor:\", X_test_tensor)\n",
    "print(\"y_train_tensor:\", y_train_tensor)\n",
    "print(\"y_test_tensor:\", y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch: tensor([[-0.2067, -0.1022,  0.3708, -0.3182,  0.1682],\n",
      "        [ 2.3863,  0.6156,  0.4132, -0.8250, -0.4078],\n",
      "        [-0.1971, -1.0594, -0.0305,  0.9488,  0.2546],\n",
      "        [-0.1989,  0.3763,  0.1555, -0.5716, -0.4366],\n",
      "        [ 0.1258, -0.1022,  0.0837,  0.1886,  0.3410],\n",
      "        [-0.1826, -0.1022,  0.2175,  0.1886, -1.3295],\n",
      "        [-0.1833,  1.5728,  0.2403, -0.8250, -0.1774],\n",
      "        [-0.1607, -1.0594, -0.6927, -0.8250, -0.0334],\n",
      "        [-0.0647, -0.3415, -0.7449, -1.0784, -1.3295],\n",
      "        [-0.1651,  1.3335,  0.6090,  1.7090,  0.6290],\n",
      "        [-0.0729,  1.1899,  1.4833,  1.2276,  1.3202],\n",
      "        [-0.1483,  3.7264,  3.1733,  0.4420,  1.1186],\n",
      "        [-0.1961,  1.3335,  0.2077,  0.1886, -0.8110],\n",
      "        [-0.1671,  0.8789,  0.7982, -1.7626, -1.4447],\n",
      "        [ 0.0777, -0.7244,  2.2892,  1.9118,  2.4435],\n",
      "        [ 0.0348, -0.1022, -2.6469,  1.4556,  1.3202],\n",
      "        [ 0.0231, -1.3705, -2.6469,  0.1633,  2.0114],\n",
      "        [ 0.1220,  0.3524,  1.6301, -0.3689, -0.0046],\n",
      "        [-0.1967, -0.5808, -0.0924,  0.6954,  0.3986],\n",
      "        [-0.0237,  0.3763, -0.3175,  1.2022,  0.2834],\n",
      "        [-0.1115,  1.3335,  1.3267,  0.6954,  1.7234],\n",
      "        [-0.0361, -1.0594, -2.6469,  0.6954,  0.1682],\n",
      "        [-0.2069, -1.2508, -0.4709, -0.6983, -0.7822],\n",
      "        [-0.1115, -0.5808, -0.1577, -1.5852, -1.3295],\n",
      "        [-0.1141, -0.3415,  0.1294, -2.0920, -1.4735],\n",
      "        [-0.1868, -0.1501,  0.7591,  3.1281,  1.4354],\n",
      "        [-0.0881, -0.6526, -0.2621,  0.6448, -0.0046],\n",
      "        [-0.1885,  0.8549,  0.3480, -0.3182, -0.8110],\n",
      "        [-0.1866,  0.6156,  0.3708, -0.3182, -1.4159],\n",
      "        [-0.1870, -0.5808,  0.2762,  1.2022,  1.1474],\n",
      "        [-0.1573,  1.3335, -0.0598, -0.0648,  0.5138],\n",
      "        [-0.0593, -0.5808, -0.6503,  0.4674,  1.3778],\n",
      "        [-0.2042, -0.4612, -0.1610, -0.3182, -1.9343],\n",
      "        [-0.0998,  1.1899,  1.8389, -0.3689, -0.8110],\n",
      "        [ 2.2478, -1.0594, -0.7025,  1.2022,  1.3202],\n",
      "        [-0.2081, -1.2987,  0.3480,  1.2276,  0.0530],\n",
      "        [ 0.1136, -0.3415, -0.1185,  0.1886,  1.0322],\n",
      "        [ 0.1297,  2.5299,  1.2778,  1.2022,  1.7234],\n",
      "        [-0.1895,  0.1371,  0.3904,  1.2022,  1.0610],\n",
      "        [-0.0848,  0.8549,  1.5355,  0.1886, -0.1198],\n",
      "        [-0.1935, -0.1022,  0.1066,  0.6954,  0.4850],\n",
      "        [-0.0409,  2.0514,  1.4572,  0.6954,  0.5426],\n",
      "        [-0.2040, -0.1022,  0.1457, -0.5716,  0.1394],\n",
      "        [-0.0254, -0.1022, -0.8852,  0.4420,  0.9458],\n",
      "        [-0.2048,  0.3763, -0.3926, -1.3318, -2.3375],\n",
      "        [-0.1651,  0.4721,  0.2566,  1.3036,  0.6866],\n",
      "        [-0.0208, -0.1022, -0.7906,  1.2022,  1.0322],\n",
      "        [-0.1218, -0.3415,  0.2175, -2.0920, -2.2799],\n",
      "        [-0.1270, -0.1022,  0.1849, -1.8386, -1.4735],\n",
      "        [ 0.0938, -2.6148, -2.1654, -0.4297, -0.3243],\n",
      "        [-0.1325, -1.3944, -0.6112,  1.3036,  0.9170],\n",
      "        [-0.1959, -1.2987,  0.3643, -0.5716, -0.6094],\n",
      "        [ 0.4141, -0.3415, -0.1218, -0.6729,  0.2546],\n",
      "        [-0.1954,  0.6156,  0.4002, -0.0648, -1.1567],\n",
      "        [-0.0747, -0.3415, -2.6469, -1.8386, -1.4735],\n",
      "        [-0.1496, -0.3415,  0.0609,  0.1886,  0.7154],\n",
      "        [-0.1857, -0.5808,  0.4230, -0.8250, -0.7822],\n",
      "        [-0.1960, -1.3944, -0.6862,  0.8475,  0.0242],\n",
      "        [-0.1824, -0.8201, -0.6144,  0.9488,  0.6578],\n",
      "        [-0.0375,  0.6156,  1.2125,  0.1886,  0.7154],\n",
      "        [-0.1824,  0.1371,  0.0707, -0.8250, -0.7246],\n",
      "        [-0.2035, -1.2030, -0.2164,  1.1516,  0.4562],\n",
      "        [-0.1872,  1.0942,  1.0918, -0.3182, -0.0334],\n",
      "        [ 0.0068, -1.2987, -2.6469,  0.4420,  1.1762]])\n",
      "y_batch: tensor([ 0.7454,  0.7758,  0.5736,  0.3733,  2.9740,  1.3475,  0.6713,  4.5587,\n",
      "         1.2857,  0.9474,  2.4206,  1.5075,  0.5798,  0.6860,  1.8180, 25.6929,\n",
      "        27.6693,  3.8947,  0.5189, 18.6642,  1.2066, 32.3364,  1.9520,  1.4375,\n",
      "         1.3473,  0.5462,  3.4300,  0.2936,  0.6590,  0.9044,  1.5612,  1.4499,\n",
      "         0.4530,  1.6810,  5.7410,  3.6800,  4.0734,  2.7200,  0.7900,  1.8880,\n",
      "         1.8951,  2.9430,  1.1653,  2.2844,  0.6650,  0.6425,  2.1779,  1.0089,\n",
      "         3.5710,  0.4047,  0.8885,  0.9075,  5.8320,  0.4194, 16.6280,  0.6204,\n",
      "         0.5102,  0.9070,  1.1850,  0.9639,  1.3343,  0.7445,  0.6142, 45.3407])\n"
     ]
    }
   ],
   "source": [
    "# Crear los datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Crear los dataloaders con batch size de 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Verificar el funcionamiento imprimiendo un batch de los datos\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(\"X_batch:\", X_batch)\n",
    "    print(\"y_batch:\", y_batch)\n",
    "    break  # Solo para imprimir el primer batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alienware X15\\anaconda3\\envs\\Apytorch\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 51.2749\n",
      "Epoch [2/50], Loss: 39.0908\n",
      "Epoch [3/50], Loss: 30.7248\n",
      "Epoch [4/50], Loss: 36.4027\n",
      "Epoch [5/50], Loss: 27.7747\n",
      "Epoch [6/50], Loss: 24.8000\n",
      "Epoch [7/50], Loss: 29.9504\n",
      "Epoch [8/50], Loss: 57.2652\n",
      "Epoch [9/50], Loss: 36.9878\n",
      "Epoch [10/50], Loss: 61.2119\n",
      "Epoch [11/50], Loss: 59.9858\n",
      "Epoch [12/50], Loss: 61.1158\n",
      "Epoch [13/50], Loss: 62.2753\n",
      "Epoch [14/50], Loss: 61.9372\n",
      "Epoch [15/50], Loss: 61.7120\n",
      "Epoch [16/50], Loss: 37.8623\n",
      "Epoch [17/50], Loss: 48.5712\n",
      "Epoch [18/50], Loss: 41.4511\n",
      "Epoch [19/50], Loss: 39.4637\n",
      "Epoch [20/50], Loss: 35.9119\n",
      "Epoch [21/50], Loss: 34.1705\n",
      "Epoch [22/50], Loss: 34.5732\n",
      "Epoch [23/50], Loss: 33.0217\n",
      "Epoch [24/50], Loss: 34.6547\n",
      "Epoch [25/50], Loss: 46.6086\n",
      "Epoch [26/50], Loss: 60.5992\n",
      "Epoch [27/50], Loss: 59.1387\n",
      "Epoch [28/50], Loss: 66.0789\n",
      "Epoch [29/50], Loss: 71.7185\n",
      "Epoch [30/50], Loss: 56.2051\n",
      "Epoch [31/50], Loss: 50.7266\n",
      "Epoch [32/50], Loss: 51.7439\n",
      "Epoch [33/50], Loss: 54.2977\n",
      "Epoch [34/50], Loss: 33.5893\n",
      "Epoch [35/50], Loss: 27.0431\n",
      "Epoch [36/50], Loss: 25.8284\n",
      "Epoch [37/50], Loss: 26.1963\n",
      "Epoch [38/50], Loss: 26.1371\n",
      "Epoch [39/50], Loss: 26.8096\n",
      "Epoch [40/50], Loss: 28.3544\n",
      "Epoch [41/50], Loss: 26.8500\n",
      "Epoch [42/50], Loss: 27.8131\n",
      "Epoch [43/50], Loss: 25.3665\n",
      "Epoch [44/50], Loss: 24.1389\n",
      "Epoch [45/50], Loss: 24.1276\n",
      "Epoch [46/50], Loss: 25.6613\n",
      "Epoch [47/50], Loss: 25.6991\n",
      "Epoch [48/50], Loss: 24.4492\n",
      "Epoch [49/50], Loss: 25.0176\n",
      "Epoch [50/50], Loss: 22.4562\n",
      "Test Loss: 24.4097\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definir el modelo Transformer\n",
    "class TransformerRegression(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
    "        super(TransformerRegression, self).__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)  # Proyecci√≥n a las dimensiones del modelo\n",
    "        self.pos_encoder = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),  # Codificaci√≥n posicional simple\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_encoder_layers, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.output_fc = nn.Linear(d_model, 1)  # Transformaci√≥n final para regresi√≥n (un valor de salida)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        # Proyectar los datos de entrada a la dimensi√≥n del modelo\n",
    "        src = self.input_fc(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        # La entrada debe tener la forma [seq_len, batch_size, feature_dim]\n",
    "        src = src.unsqueeze(0)  # A√±adimos una dimensi√≥n de secuencia (de longitud 1 en este caso)\n",
    "        \n",
    "        # Pasamos los datos a trav√©s del transformer\n",
    "        output = self.transformer(src, src)\n",
    "        \n",
    "        # Tomamos solo el √∫ltimo vector para la predicci√≥n\n",
    "        output = self.output_fc(output.squeeze(0))\n",
    "        return output\n",
    "\n",
    "# Inicializar el modelo\n",
    "input_dim = X_train_tensor.shape[1]  # Dimensi√≥n de entrada (n√∫mero de caracter√≠sticas)\n",
    "d_model = 128  # Tama√±o del embedding\n",
    "nhead = 4  # N√∫mero de cabezas de atenci√≥n\n",
    "num_encoder_layers = 3  # N√∫mero de capas del encoder\n",
    "dim_feedforward = 128  # Tama√±o de las capas feedforward\n",
    "dropout = 0.1  # Tasa de dropout\n",
    "\n",
    "model = TransformerRegression(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "# Definir el optimizador y la funci√≥n de p√©rdida\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "def train_model(model, train_loader, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.squeeze(-1), y_batch)  # Ajuste para eliminar la dimensi√≥n extra de salida\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Entrenar el modelo por 50 epochs\n",
    "num_epochs = 50\n",
    "train_model(model, train_loader, num_epochs)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.squeeze(-1), y_batch)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    print(f'Test Loss: {total_loss/len(test_loader):.4f}')\n",
    "\n",
    "# Evaluar en el conjunto de prueba\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 55.8563\n",
      "Epoch [2/50], Loss: 43.4895\n",
      "Epoch [3/50], Loss: 33.4976\n",
      "Epoch [4/50], Loss: 33.7798\n",
      "Epoch [5/50], Loss: 24.9737\n",
      "Epoch [6/50], Loss: 24.8261\n",
      "Epoch [7/50], Loss: 30.0731\n",
      "Epoch [8/50], Loss: 23.1109\n",
      "Epoch [9/50], Loss: 29.7986\n",
      "Epoch [10/50], Loss: 22.9058\n",
      "Epoch [11/50], Loss: 35.4626\n",
      "Epoch [12/50], Loss: 26.5916\n",
      "Epoch [13/50], Loss: 22.0304\n",
      "Epoch [14/50], Loss: 29.0998\n",
      "Epoch [15/50], Loss: 26.3026\n",
      "Epoch [16/50], Loss: 23.9389\n",
      "Epoch [17/50], Loss: 23.1260\n",
      "Epoch [18/50], Loss: 24.6683\n",
      "Epoch [19/50], Loss: 23.9188\n",
      "Epoch [20/50], Loss: 26.4518\n",
      "Epoch [21/50], Loss: 41.8945\n",
      "Epoch [22/50], Loss: 32.9100\n",
      "Epoch [23/50], Loss: 32.6572\n",
      "Epoch [24/50], Loss: 33.1944\n",
      "Epoch [25/50], Loss: 33.0145\n",
      "Epoch [26/50], Loss: 32.9621\n",
      "Epoch [27/50], Loss: 32.3990\n",
      "Epoch [28/50], Loss: 34.3720\n",
      "Epoch [29/50], Loss: 32.9623\n",
      "Epoch [30/50], Loss: 33.0150\n",
      "Epoch [31/50], Loss: 33.6679\n",
      "Epoch [32/50], Loss: 33.7875\n",
      "Epoch [33/50], Loss: 32.8325\n",
      "Epoch [34/50], Loss: 32.4313\n",
      "Epoch [35/50], Loss: 31.8949\n",
      "Epoch [36/50], Loss: 32.0041\n",
      "Epoch [37/50], Loss: 32.1520\n",
      "Epoch [38/50], Loss: 32.2940\n",
      "Epoch [39/50], Loss: 32.3118\n",
      "Epoch [40/50], Loss: 31.9409\n",
      "Epoch [41/50], Loss: 32.7110\n",
      "Epoch [42/50], Loss: 32.0306\n",
      "Epoch [43/50], Loss: 32.1317\n",
      "Epoch [44/50], Loss: 32.2651\n",
      "Epoch [45/50], Loss: 30.8296\n",
      "Epoch [46/50], Loss: 31.3387\n",
      "Epoch [47/50], Loss: 31.1605\n",
      "Epoch [48/50], Loss: 31.1693\n",
      "Epoch [49/50], Loss: 29.1677\n",
      "Epoch [50/50], Loss: 27.6486\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 96\u001b[0m\n\u001b[0;32m     93\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Evaluar y graficar en el conjunto de prueba\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m \u001b[43mevaluate_and_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 82\u001b[0m, in \u001b[0;36mevaluate_and_plot\u001b[1;34m(model, test_loader)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     81\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[1;32m---> 82\u001b[0m         real_values\u001b[38;5;241m.\u001b[39mextend(\u001b[43my_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Convertir los valores reales a una lista\u001b[39;00m\n\u001b[0;32m     83\u001b[0m         predicted_values\u001b[38;5;241m.\u001b[39mextend(outputs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# Convertir las predicciones a una lista\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Graficar los valores reales vs predichos\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definir el modelo Transformer\n",
    "class TransformerRegression(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
    "        super(TransformerRegression, self).__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)  # Proyecci√≥n a las dimensiones del modelo\n",
    "        self.pos_encoder = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),  # Codificaci√≥n posicional simple\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_encoder_layers, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.output_fc = nn.Linear(d_model, 1)  # Transformaci√≥n final para regresi√≥n (un valor de salida)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        # Proyectar los datos de entrada a la dimensi√≥n del modelo\n",
    "        src = self.input_fc(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        # La entrada debe tener la forma [seq_len, batch_size, feature_dim]\n",
    "        src = src.unsqueeze(0)  # A√±adimos una dimensi√≥n de secuencia (de longitud 1 en este caso)\n",
    "        \n",
    "        # Pasamos los datos a trav√©s del transformer\n",
    "        output = self.transformer(src, src)\n",
    "        \n",
    "        # Tomamos solo el √∫ltimo vector para la predicci√≥n\n",
    "        output = self.output_fc(output.squeeze(0))\n",
    "        return output\n",
    "\n",
    "# Inicializar el modelo\n",
    "input_dim = X_train_tensor.shape[1]  # Dimensi√≥n de entrada (n√∫mero de caracter√≠sticas)\n",
    "d_model = 64  # Tama√±o del embedding\n",
    "nhead = 4  # N√∫mero de cabezas de atenci√≥n\n",
    "num_encoder_layers = 3  # N√∫mero de capas del encoder\n",
    "dim_feedforward = 128  # Tama√±o de las capas feedforward\n",
    "dropout = 0.1  # Tasa de dropout\n",
    "\n",
    "model = TransformerRegression(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "# Definir el optimizador y la funci√≥n de p√©rdida\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "def train_model(model, train_loader, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.squeeze(-1), y_batch)  # Ajuste para eliminar la dimensi√≥n extra de salida\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Entrenar el modelo por 50 epochs\n",
    "num_epochs = 50\n",
    "train_model(model, train_loader, num_epochs)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Funci√≥n para evaluar el modelo y graficar los valores reales vs. predichos\n",
    "def evaluate_and_plot(model, test_loader):\n",
    "    model.eval()\n",
    "    real_values = []\n",
    "    predicted_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            real_values.extend(y_batch.numpy())  # Convertir los valores reales a una lista\n",
    "            predicted_values.extend(outputs.squeeze(-1).numpy())  # Convertir las predicciones a una lista\n",
    "\n",
    "    # Graficar los valores reales vs predichos\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(real_values, predicted_values, alpha=0.5, color='b', label='Predicciones')\n",
    "    plt.plot([min(real_values), max(real_values)], [min(real_values), max(real_values)], color='r', label='L√≠nea ideal')  # L√≠nea de referencia\n",
    "    plt.xlabel('Valores Reales')\n",
    "    plt.ylabel('Valores Predichos')\n",
    "    plt.title('Valores Reales vs Predichos')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluar y graficar en el conjunto de prueba\n",
    "evaluate_and_plot(model, test_loader)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
